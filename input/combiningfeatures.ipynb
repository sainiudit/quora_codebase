{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 20 11:23:59 2017\n",
    "\n",
    "@author: mariosm\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#stops = set(stopwords.words(\"english\"))\n",
    "stops = set([\"http\",\"www\",\"img\",\"border\",\"home\",\"body\",\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\n",
    "\"and\",\"any\",\"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\n",
    "\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\n",
    "\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\n",
    "\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\n",
    "\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\n",
    "\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\n",
    "\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\n",
    "\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\n",
    "\"weren't\",\"what\",\"what's\",\"when\",\"when's\"\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\n",
    "\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\" ])\n",
    "\n",
    "weights={}\n",
    "\n",
    "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):    \n",
    "    zsparse=csr_matrix(array)\n",
    "    indptr = zsparse.indptr\n",
    "    indices = zsparse.indices\n",
    "    data = zsparse.data\n",
    "    print(\" data lenth %d\" % (len(data)))\n",
    "    print(\" indices lenth %d\" % (len(indices)))    \n",
    "    print(\" indptr lenth %d\" % (len(indptr)))\n",
    "    \n",
    "    f=open(filename,\"w\")\n",
    "    counter_row=0\n",
    "    for b in range(0,len(indptr)-1):\n",
    "        #if there is a target, print it else , print nothing\n",
    "        if ytarget!=None:\n",
    "             f.write(str(ytarget[b]) + deli1)     \n",
    "             \n",
    "        for k in range(indptr[b],indptr[b+1]):\n",
    "            if (k==indptr[b]):\n",
    "                if np.isnan(data[k]):\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))                    \n",
    "            else :\n",
    "                if np.isnan(data[k]):\n",
    "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))  \n",
    "                else :\n",
    "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n",
    "        f.write(\"\\n\")\n",
    "        counter_row+=1\n",
    "        if counter_row%10000==0:    \n",
    "            print(\" row : %d \" % (counter_row))    \n",
    "    f.close()  \n",
    "    \n",
    "\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=5000.0, min_count=2.0):\n",
    "    if count < min_count:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0 / (count + eps)\n",
    "\n",
    "\n",
    "def word_shares(row,wei,stop):\n",
    "\t\tq1 = set(str(row['question1']).lower().split())\n",
    "\t\tq1words = q1.difference(stop)\n",
    "\t\tif len(q1words) == 0:\n",
    "\t\t\treturn '0:0:0:0:0'\n",
    "\n",
    "\t\tq2 = set(str(row['question2']).lower().split())\n",
    "\t\tq2words = q2.difference(stop)\n",
    "\t\tif len(q2words) == 0:\n",
    "\t\t\treturn '0:0:0:0:0'\n",
    "\n",
    "\t\tq1stops = q1.intersection(stop)\n",
    "\t\tq2stops = q2.intersection(stop)\n",
    "\n",
    "\t\tshared_words = q1words.intersection(q2words)\n",
    "\t\t#print(len(shared_words))\n",
    "\t\tshared_weights = [wei.get(w, 0) for w in shared_words]\n",
    "\t\ttotal_weights = [wei.get(w, 0) for w in q1words] + [wei.get(w, 0) for w in q2words]\n",
    "        \n",
    "\t\tR1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "\t\tR2 = float(len(shared_words)) / (float(len(q1words)) + float(len(q2words))) #count share\n",
    "\t\tR31 = float(len(q1stops)) / float(len(q1words)) #stops in q1\n",
    "\t\tR32 = float(len(q2stops)) / float(len(q2words)) #stops in q2\n",
    "\t\treturn '{}:{}:{}:{}:{}'.format(R1, R2, float(len(shared_words)), R31, R32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: X_train: (404290, 6), X_test: (2345796, 3)\n",
      "Features processing, be patient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:99: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:99: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.20263606748063515, 0.20662482868938806, 0.13554092610089866, 0.13679914426398324)\n"
     ]
    }
   ],
   "source": [
    "input_folder=\"../input/\" # set your input folder here\n",
    "df_train = pd.read_csv(input_folder + 'train.csv')\n",
    "df_test  = pd.read_csv(input_folder + 'test.csv')\n",
    "print(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n",
    "\n",
    "\n",
    "train_mix = (df_train['question1']+ \" \" +  df_train['question2']).astype(str).values\n",
    "test_mix = (df_test['question1']+ \" \" +  df_test['question2'] ).astype(str).values   \t\n",
    "print(\"Features processing, be patient...\")\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "#stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X_test = pd.DataFrame()\n",
    "df_train['word_shares'] = df_train.apply(word_shares, args = (weights,stops,),axis=1, raw=True)\n",
    "df_test['word_shares'] = df_test.apply(word_shares, args = (weights,stops,),axis=1, raw=True)\n",
    "\n",
    "X['word_match']       = df_train['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "X['tfidf_word_match'] = df_train['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "X['shared_count']     = df_train['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "X['stops1_ratio']     = df_train['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "X['stops2_ratio']     = df_train['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "X['diff_stops_r']     = X['stops1_ratio'] - X['stops2_ratio']\n",
    "X['len_q1'] = df_train['question1'].apply(lambda x: len(str(x)))\n",
    "X['len_q2'] = df_train['question2'].apply(lambda x: len(str(x)))\n",
    "X['diff_len'] = X['len_q1'] - X['len_q2']\n",
    "X['len_char_q1'] = df_train['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "X['len_char_q2'] = df_train['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "X['diff_len_char'] = X['len_char_q1'] - X['len_char_q2']\n",
    "X['len_word_q1'] = df_train['question1'].apply(lambda x: len(str(x).split()))\n",
    "X['len_word_q2'] = df_train['question2'].apply(lambda x: len(str(x).split()))\n",
    "X['diff_len_word'] = X['len_word_q1'] - X['len_word_q2']\n",
    "X['avg_world_len1'] = X['len_char_q1'] / X['len_word_q1']\n",
    "X['avg_world_len2'] = X['len_char_q2'] / X['len_word_q2']\n",
    "X['diff_avg_word'] = X['avg_world_len1'] - X['avg_world_len2']\n",
    "X['exactly_same'] = (df_train['question1'] == df_train['question2']).astype(int)\n",
    "\n",
    "X_test['word_match']       = df_test['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "X_test['tfidf_word_match'] = df_test['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "X_test['shared_count']     = df_test['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "X_test['stops1_ratio']     = df_test['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "X_test['stops2_ratio']     = df_test['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "X_test['diff_stops_r']     = X_test['stops1_ratio'] - X_test['stops2_ratio']\n",
    "X_test['len_q1'] = df_test['question1'].apply(lambda x: len(str(x)))\n",
    "X_test['len_q2'] = df_test['question2'].apply(lambda x: len(str(x)))\n",
    "X_test['diff_len'] = X_test['len_q1'] - X_test['len_q2']\n",
    "X_test['len_char_q1'] = df_test['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "X_test['len_char_q2'] = df_test['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "X_test['diff_len_char'] = X_test['len_char_q1'] - X_test['len_char_q2']\n",
    "X_test['len_word_q1'] = df_test['question1'].apply(lambda x: len(str(x).split()))\n",
    "X_test['len_word_q2'] = df_test['question2'].apply(lambda x: len(str(x).split()))\n",
    "X_test['diff_len_word'] = X_test['len_word_q1'] - X_test['len_word_q2']\n",
    "X_test['avg_world_len1'] = X_test['len_char_q1'] / X_test['len_word_q1']\n",
    "X_test['avg_world_len2'] = X_test['len_char_q2'] / X_test['len_word_q2']\n",
    "X_test['diff_avg_word'] = X_test['avg_world_len1'] - X_test['avg_world_len2']\n",
    "X_test['exactly_same'] = (df_test['question1'] == df_test['question2']).astype(int)   \n",
    "print (np.mean(X['word_match']) , np.mean(X['tfidf_word_match']),np.mean(X_test['word_match']) , np.mean(X_test['tfidf_word_match']))\n",
    "#convert to csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.to_csv(\"../../input/rader/train_base_features.csv\")\n",
    "X_test.to_csv(\"../../input/rader/test_base_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abhi_test=pd.read_csv(\"../../input/rader/test_features.csv\")\n",
    "abhi_train=pd.read_csv(\"../../input/rader/train_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=['stops1_ratio','tfidf_word_match','stops2_ratio','diff_stops_r','diff_len','diff_len_char','diff_len_word','avg_world_len1','avg_world_len2','diff_avg_word','exactly_same']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    abhi_train.loc[:,col]=X[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    abhi_test.loc[:,col]=X_test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abhi_train.to_csv(\"abhi_andmine_train.csv\",index=False)\n",
    "abhi_test.to_csv(\"abhi_andmine_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abhi_train=pd.read_csv(\"abhi_andmine_train.csv\")\n",
    "abhi_test=pd.read_csv(\"abhi_andmine_test.csv\")\n",
    "features=abhi_test.columns.tolist()\n",
    "features.remove(\"question1\")\n",
    "features.remove(\"question2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abhi_train=abhi_train.fillna(0)\n",
    "abhi_test=abhi_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\n",
    "from sklearn.utils import resample,shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"train.csv\",usecols=['id','is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:18: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:19: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:26: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:27: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:32: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:33: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "path=\"/home/udit/ipython/notebook/quora/input/input/\"\n",
    "train = pd.read_csv(path+\"train_porter.csv\")    \n",
    "\n",
    "train_question1_tfidf = pd.read_pickle(path+'train_question1_tfidf.pkl')[:]\n",
    "test_question1_tfidf = pd.read_pickle(path+'test_question1_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_question2_tfidf = pd.read_pickle(path+'train_question2_tfidf.pkl')[:]\n",
    "test_question2_tfidf = pd.read_pickle(path+'test_question2_tfidf.pkl')[:]\n",
    "\n",
    "train_interaction = pd.read_pickle(path+'train_interaction.pkl')[:].reshape(-1,1)\n",
    "test_interaction = pd.read_pickle(path+'test_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_interaction=np.nan_to_num(train_interaction)\n",
    "test_interaction=np.nan_to_num(test_interaction)      \n",
    "\n",
    "\n",
    "train_porter_interaction = pd.read_pickle(path+'train_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "test_porter_interaction = pd.read_pickle(path+'test_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "train_porter_interaction=np.nan_to_num(train_porter_interaction)\n",
    "test_porter_interaction=np.nan_to_num(test_porter_interaction)\n",
    "\n",
    "\n",
    "train_jaccard = pd.read_pickle(path+'train_jaccard.pkl')[:].reshape(-1,1)\n",
    "test_jaccard = pd.read_pickle(path+'test_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_jaccard=np.nan_to_num(train_jaccard)\n",
    "test_jaccard=np.nan_to_num(test_jaccard)\n",
    "\n",
    "train_porter_jaccard = pd.read_pickle(path+'train_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "test_porter_jaccard = pd.read_pickle(path+'test_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "train_jaccard=np.nan_to_num(train_jaccard)\n",
    "test_porter_jaccard=np.nan_to_num(test_porter_jaccard)\n",
    "\n",
    "train_len = pd.read_pickle(path+\"train_len.pkl\")\n",
    "test_len = pd.read_pickle(path+\"test_len.pkl\")\n",
    "\n",
    "train_len=np.nan_to_num(train_len)\n",
    "test_len=np.nan_to_num(test_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as ssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=csr_matrix(abhi_train[features])\n",
    "X_test=csr_matrix(abhi_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(min_df=1, max_features=None, strip_accents='unicode',lowercase =True,\n",
    "                    analyzer='word', token_pattern=r'\\w{2,}', ngram_range=(1, 1), use_idf=True,smooth_idf=True, \n",
    "sublinear_tf=True, stop_words = 'english')  \n",
    "\n",
    "# aplied tf-idf\n",
    "tr_sparsed  = tfidf.fit_transform (train_mix)  \n",
    "te_sparsed = tfidf.transform(test_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del abhi_train\n",
    "del abhi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fromsparsetofile(\"train_intraction.sparse\", X, deli1=\" \", deli2=\":\",ytarget=y)    \n",
    "fromsparsetofile(\"test_intraction.sparse\", X_t, deli1=\" \", deli2=\":\",ytarget=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.vstack([train_len,test_len]))\n",
    "train_len = scaler.transform(train_len)\n",
    "test_len =scaler.transform(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((404290, 85109), (2345796, 85109), (404290, 38), (2345796, 38))\n",
      " data lenth 17551213\n",
      " indices lenth 17551213\n",
      " indptr lenth 404291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:49: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " row : 10000 \n",
      " row : 20000 \n",
      " row : 30000 \n",
      " row : 40000 \n",
      " row : 50000 \n",
      " row : 60000 \n",
      " row : 70000 \n",
      " row : 80000 \n",
      " row : 90000 \n",
      " row : 100000 \n",
      " row : 110000 \n",
      " row : 120000 \n",
      " row : 130000 \n",
      " row : 140000 \n",
      " row : 150000 \n",
      " row : 160000 \n",
      " row : 170000 \n",
      " row : 180000 \n",
      " row : 190000 \n",
      " row : 200000 \n",
      " row : 210000 \n",
      " row : 220000 \n",
      " row : 230000 \n",
      " row : 240000 \n",
      " row : 250000 \n",
      " row : 260000 \n",
      " row : 270000 \n",
      " row : 280000 \n",
      " row : 290000 \n",
      " row : 300000 \n",
      " row : 310000 \n",
      " row : 320000 \n",
      " row : 330000 \n",
      " row : 340000 \n",
      " row : 350000 \n",
      " row : 360000 \n",
      " row : 370000 \n",
      " row : 380000 \n",
      " row : 390000 \n",
      " row : 400000 \n",
      " data lenth 104716222\n",
      " indices lenth 104716222\n",
      " indptr lenth 2345797\n",
      " row : 10000 \n",
      " row : 20000 \n",
      " row : 30000 \n",
      " row : 40000 \n",
      " row : 50000 \n",
      " row : 60000 \n",
      " row : 70000 \n",
      " row : 80000 \n",
      " row : 90000 \n",
      " row : 100000 \n",
      " row : 110000 \n",
      " row : 120000 \n",
      " row : 130000 \n",
      " row : 140000 \n",
      " row : 150000 \n",
      " row : 160000 \n",
      " row : 170000 \n",
      " row : 180000 \n",
      " row : 190000 \n",
      " row : 200000 \n",
      " row : 210000 \n",
      " row : 220000 \n",
      " row : 230000 \n",
      " row : 240000 \n",
      " row : 250000 \n",
      " row : 260000 \n",
      " row : 270000 \n",
      " row : 280000 \n",
      " row : 290000 \n",
      " row : 300000 \n",
      " row : 310000 \n",
      " row : 320000 \n",
      " row : 330000 \n",
      " row : 340000 \n",
      " row : 350000 \n",
      " row : 360000 \n",
      " row : 370000 \n",
      " row : 380000 \n",
      " row : 390000 \n",
      " row : 400000 \n",
      " row : 410000 \n",
      " row : 420000 \n",
      " row : 430000 \n",
      " row : 440000 \n",
      " row : 450000 \n",
      " row : 460000 \n",
      " row : 470000 \n",
      " row : 480000 \n",
      " row : 490000 \n",
      " row : 500000 \n",
      " row : 510000 \n",
      " row : 520000 \n",
      " row : 530000 \n",
      " row : 540000 \n",
      " row : 550000 \n",
      " row : 560000 \n",
      " row : 570000 \n",
      " row : 580000 \n",
      " row : 590000 \n",
      " row : 600000 \n",
      " row : 610000 \n",
      " row : 620000 \n",
      " row : 630000 \n",
      " row : 640000 \n",
      " row : 650000 \n",
      " row : 660000 \n",
      " row : 670000 \n",
      " row : 680000 \n",
      " row : 690000 \n",
      " row : 700000 \n",
      " row : 710000 \n",
      " row : 720000 \n",
      " row : 730000 \n",
      " row : 740000 \n",
      " row : 750000 \n",
      " row : 760000 \n",
      " row : 770000 \n",
      " row : 780000 \n",
      " row : 790000 \n",
      " row : 800000 \n",
      " row : 810000 \n",
      " row : 820000 \n",
      " row : 830000 \n",
      " row : 840000 \n",
      " row : 850000 \n",
      " row : 860000 \n",
      " row : 870000 \n",
      " row : 880000 \n",
      " row : 890000 \n",
      " row : 900000 \n",
      " row : 910000 \n",
      " row : 920000 \n",
      " row : 930000 \n",
      " row : 940000 \n",
      " row : 950000 \n",
      " row : 960000 \n",
      " row : 970000 \n",
      " row : 980000 \n",
      " row : 990000 \n",
      " row : 1000000 \n",
      " row : 1010000 \n",
      " row : 1020000 \n",
      " row : 1030000 \n",
      " row : 1040000 \n",
      " row : 1050000 \n",
      " row : 1060000 \n",
      " row : 1070000 \n",
      " row : 1080000 \n",
      " row : 1090000 \n",
      " row : 1100000 \n",
      " row : 1110000 \n",
      " row : 1120000 \n",
      " row : 1130000 \n",
      " row : 1140000 \n",
      " row : 1150000 \n",
      " row : 1160000 \n",
      " row : 1170000 \n",
      " row : 1180000 \n",
      " row : 1190000 \n",
      " row : 1200000 \n",
      " row : 1210000 \n",
      " row : 1220000 \n",
      " row : 1230000 \n",
      " row : 1240000 \n",
      " row : 1250000 \n",
      " row : 1260000 \n",
      " row : 1270000 \n",
      " row : 1280000 \n",
      " row : 1290000 \n",
      " row : 1300000 \n",
      " row : 1310000 \n",
      " row : 1320000 \n",
      " row : 1330000 \n",
      " row : 1340000 \n",
      " row : 1350000 \n",
      " row : 1360000 \n",
      " row : 1370000 \n",
      " row : 1380000 \n",
      " row : 1390000 \n",
      " row : 1400000 \n",
      " row : 1410000 \n",
      " row : 1420000 \n",
      " row : 1430000 \n",
      " row : 1440000 \n",
      " row : 1450000 \n",
      " row : 1460000 \n",
      " row : 1470000 \n",
      " row : 1480000 \n",
      " row : 1490000 \n",
      " row : 1500000 \n",
      " row : 1510000 \n",
      " row : 1520000 \n",
      " row : 1530000 \n",
      " row : 1540000 \n",
      " row : 1550000 \n",
      " row : 1560000 \n",
      " row : 1570000 \n",
      " row : 1580000 \n",
      " row : 1590000 \n",
      " row : 1600000 \n",
      " row : 1610000 \n",
      " row : 1620000 \n",
      " row : 1630000 \n",
      " row : 1640000 \n",
      " row : 1650000 \n",
      " row : 1660000 \n",
      " row : 1670000 \n",
      " row : 1680000 \n",
      " row : 1690000 \n",
      " row : 1700000 \n",
      " row : 1710000 \n",
      " row : 1720000 \n",
      " row : 1730000 \n",
      " row : 1740000 \n",
      " row : 1750000 \n",
      " row : 1760000 \n",
      " row : 1770000 \n",
      " row : 1780000 \n",
      " row : 1790000 \n",
      " row : 1800000 \n",
      " row : 1810000 \n",
      " row : 1820000 \n",
      " row : 1830000 \n",
      " row : 1840000 \n",
      " row : 1850000 \n",
      " row : 1860000 \n",
      " row : 1870000 \n",
      " row : 1880000 \n",
      " row : 1890000 \n",
      " row : 1900000 \n",
      " row : 1910000 \n",
      " row : 1920000 \n",
      " row : 1930000 \n",
      " row : 1940000 \n",
      " row : 1950000 \n",
      " row : 1960000 \n",
      " row : 1970000 \n",
      " row : 1980000 \n",
      " row : 1990000 \n",
      " row : 2000000 \n",
      " row : 2010000 \n",
      " row : 2020000 \n",
      " row : 2030000 \n",
      " row : 2040000 \n",
      " row : 2050000 \n",
      " row : 2060000 \n",
      " row : 2070000 \n",
      " row : 2080000 \n",
      " row : 2090000 \n",
      " row : 2100000 \n",
      " row : 2110000 \n",
      " row : 2120000 \n",
      " row : 2130000 \n",
      " row : 2140000 \n",
      " row : 2150000 \n",
      " row : 2160000 \n",
      " row : 2170000 \n",
      " row : 2180000 \n",
      " row : 2190000 \n",
      " row : 2200000 \n",
      " row : 2210000 \n",
      " row : 2220000 \n",
      " row : 2230000 \n",
      " row : 2240000 \n",
      " row : 2250000 \n",
      " row : 2260000 \n",
      " row : 2270000 \n",
      " row : 2280000 \n",
      " row : 2290000 \n",
      " row : 2300000 \n",
      " row : 2310000 \n",
      " row : 2320000 \n",
      " row : 2330000 \n",
      " row : 2340000 \n"
     ]
    }
   ],
   "source": [
    "X=csr_matrix(abhi_train[features])\n",
    "X_test=csr_matrix(abhi_test[features])\n",
    "# the tfidf object\n",
    "tfidf=TfidfVectorizer(min_df=1, max_features=None, strip_accents='unicode',lowercase =True,\n",
    "                    analyzer='word', token_pattern=r'\\w{2,}', ngram_range=(1, 1), use_idf=True,smooth_idf=True, \n",
    "sublinear_tf=True, stop_words = 'english')  \n",
    "\n",
    "# aplied tf-idf\n",
    "tr_sparsed  = tfidf.fit_transform (train_mix)  \n",
    "te_sparsed = tfidf.transform(test_mix)\n",
    "print (tr_sparsed.shape, te_sparsed.shape, X.shape, X_test.shape)  \n",
    "#join the the tfidf with the remaining data\n",
    "\n",
    "\n",
    "X =hstack([X,tr_sparsed]).tocsr()#\n",
    "X_test = hstack([X_test, te_sparsed]).tocsr()#\n",
    "\n",
    "#retrieve target\n",
    "y = df_train['is_duplicate'].values  \n",
    "#print (X.shape, X_test.shape, y.shape)\n",
    "\n",
    "#export sparse data to stacknet format (which is Libsvm format)\n",
    "fromsparsetofile(\"train.sparse\", X, deli1=\" \", deli2=\":\",ytarget=y)    \n",
    "fromsparsetofile(\"test.sparse\", X_test, deli1=\" \", deli2=\":\",ytarget=None)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
