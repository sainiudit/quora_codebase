{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#matplotlib.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv('../input/train.csv')\n",
    "trainDF = trainDF.dropna(how=\"any\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureExtractionStartTime = time.time()\n",
    "\n",
    "maxNumFeatures = 300000\n",
    "\n",
    "# bag of letter sequences (chars)\n",
    "BagOfWordsExtractor = CountVectorizer(max_df=0.999, min_df=50, max_features=maxNumFeatures, \n",
    "                                      analyzer='char', ngram_range=(1,6), \n",
    "                                      binary=True, lowercase=True)\n",
    "# bag of words\n",
    "#BagOfWordsExtractor = CountVectorizer(max_df=0.999, min_df=10, max_features=maxNumFeatures, \n",
    "#                                      analyzer='word', ngram_range=(1,6), stop_words='english', \n",
    "#                                      binary=True, lowercase=True)\n",
    "\n",
    "BagOfWordsExtractor.fit(pd.concat((trainDF.ix[:,'question1'],trainDF.ix[:,'question2'])).unique())\n",
    "\n",
    "trainQuestion1_BOW_rep = BagOfWordsExtractor.transform(trainDF.ix[:,'question1'])\n",
    "trainQuestion2_BOW_rep = BagOfWordsExtractor.transform(trainDF.ix[:,'question2'])\n",
    "lables = np.array(trainDF.ix[:,'is_duplicate'])\n",
    "\n",
    "featureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0\n",
    "print(\"feature extraction took %.2f minutes\" % (featureExtractionDurationInMinutes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "a = 0.165 / 0.37\n",
    "b = (1 - 0.165) / (1 - 0.37)\n",
    "\n",
    "def kappa(preds, y):\n",
    "    score = []\n",
    "    for pp,yy in zip(preds, y.get_label()):\n",
    "        score.append(a * yy * np.log (pp) + b * (1 - yy) * np.log(1-pp))\n",
    "    score = -np.sum(score) / len(score)\n",
    "\n",
    "    return 'kappa', float(score),False\n",
    "\n",
    "\n",
    "def run_lgb_native(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=500000,e_stoping_r=50):\n",
    "   \n",
    "    params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    #'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_freq': 7,\n",
    "    'verbose': 0,\n",
    "    #'regression_l1':107\n",
    "    #'scale_pos_weight':1.36,\n",
    "       # 'is_unbalance':True\n",
    "        }\n",
    "    if test_y is not None:\n",
    "        lgb_train = lgbm.Dataset(train_X, train_y,\n",
    "                        free_raw_data=False)\n",
    "        lgb_eval = lgbm.Dataset(test_X, test_y, reference=lgb_train,\n",
    "                        free_raw_data=False)\n",
    "        model = lgbm.train(params,lgb_train, num_boost_round=num_rounds, feval=kappa,valid_sets=lgb_eval,verbose_eval=10)\n",
    "        #model.fit(train_X,train_y,eval_set=[(train_X,train_y),(test_X, test_y)],verbose=100,early_stopping_rounds=e_stoping_r,eval_metric=fun_loss)\n",
    "    else:\n",
    "        lgb_train = lgbm.Dataset(train_X, train_y,\n",
    "                        free_raw_data=False)\n",
    "        model=lgbm.train(params,lgb_train,num_boost_round=num_rounds, feval=kappa)\n",
    "        \n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = -(trainQuestion1_BOW_rep != trainQuestion2_BOW_rep).astype(int)\n",
    "#X = -(trainQuestion1_BOW_rep != trainQuestion2_BOW_rep).astype(int) + \\\n",
    "#      trainQuestion1_BOW_rep.multiply(trainQuestion2_BOW_rep)\n",
    "y = lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "preds, model = run_lgb_native(X_train, y_train,X_test,y_test,num_rounds=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
