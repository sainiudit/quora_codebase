{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "import cPickle as pickle\n",
    "\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "#from tsne import bh_sne\n",
    "from gensim.models import Word2Vec\n",
    "stops = set([\"http\",\"www\",\"img\",\"border\",\"home\",\"body\",\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\n",
    "\"and\",\"any\",\"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\n",
    "\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\n",
    "\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\n",
    "\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\n",
    "\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\n",
    "\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\n",
    "\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\n",
    "\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\n",
    "\"weren't\",\"what\",\"what's\",\"when\",\"when's\"\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\n",
    "\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\" ])\n",
    "\n",
    "train_df = pd.read_csv(\"train_porter.csv\", encoding='utf-8').fillna('')\n",
    "test_df  = pd.read_csv(\"test_porter.csv\", encoding='utf-8').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def calc_cosine_dist(text_a ,text_b, vect):\n",
    "    text_a=re.sub(r'[^\\x00-\\x7f]',r' ',text_a)\n",
    "    text_b=re.sub(r'[^\\x00-\\x7f]',r' ',text_b)\n",
    "    return pairwise_distances(vect.transform([text_a]), vect.transform([text_b]), metric='cosine')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv_orig = TfidfVectorizer(ngram_range=(1,2), min_df=2,stop_words=stops,max_features=2000)\n",
    "tfv_stem = TfidfVectorizer(ngram_range=(1,2), min_df=2,stop_words=stops,max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=200, min_df=2,\n",
       "        ngram_range=(1, 2), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=set(['all', \"she'll\", \"don't\", 'being', 'over', 'through', 'yourselves', 'its', 'before', 'img', \"he's\", \"we've\", 'had', 'should', \"he'd\", 'to', 'only', \"there's\", 'those', 'under', 'ours', 'has', \"haven't\", 'do', 'them', 'his', \"they'll\", 'very', \"who's\", \"they'd\", 'cannot', \"you've\", 't...ff', 'home', \"she'd\", 'yours', \"you'll\", 'so', \"we're\", \"she's\", 'the', \"that's\", 'having', 'once']),\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv_orig.fit(\n",
    "    list(train_df['question1'].values) + \n",
    "    list(test_df['question1'].values)+\n",
    "    list(train_df['question2'].values) + \n",
    "    list(test_df['question2'].values)\n",
    ") \n",
    "tfv_stem.fit(\n",
    "    list(train_df['question1_porter'].values) + \n",
    "    list(test_df['question1_porter'].values)+\n",
    "    list(train_df['question2_porter'].values) + \n",
    "    list(test_df['question2_porter'].values)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"/home/udit/ipython/notebook/quora/input/input/\"\n",
    "train_cosine = train_df.apply(lambda x:calc_cosine_dist(x['question1'],x['question2'],tfv_orig),axis=1)\n",
    "pd.to_pickle(train_cosine,path+\"train_cosine_dist.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_cosine = test_df.apply(lambda x:calc_cosine_dist(x['question1'],x['question2'],tfv_orig),axis=1)\n",
    "pd.to_pickle(test_cosine,path+\"test_cosine_dist.pkl\")\n",
    "\n",
    "print('Generate porter levenshtein_2')\n",
    "train_porter_cosine = train_df.apply(lambda x:calc_cosine_dist(x['question1_porter'],x['question2_porter'],tfv_stem),axis=1)\n",
    "test_porter_cosine= test_df.apply(lambda x:calc_cosine_dist(x['question1_porter'],x['question2_porter'],tfv_stem),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_cosine,path+\"train_porter_cosine_dist.pkl\")\n",
    "pd.to_pickle(test_porter_cosine,path+\"test_porter_cosine_dist.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
