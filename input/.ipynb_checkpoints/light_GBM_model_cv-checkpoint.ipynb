{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgbm\n",
    "def runlgbm(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=500000,e_stoping_r=50): \n",
    "    t4_params = {\n",
    "        'boosting_type': 'gbdt', 'objective': 'binary', 'nthread': -1, 'silent': True,\n",
    "        'num_leaves': 6, 'learning_rate': 0.03, 'max_depth': 6,\n",
    "        'max_bin': 255, 'subsample_for_bin': 50000,\n",
    "        'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.7, 'reg_alpha':1, 'reg_lambda':0,\n",
    "        'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight': 1}\n",
    "\n",
    "    # they can be used directly to build a LGBMClassifier (which is wrapped in a sklearn fashion)\n",
    "    model = lgbm.sklearn.LGBMClassifier(n_estimators=num_rounds, seed=0, **t4_params)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        model.fit(train_X,train_y,eval_set=[(train_X,train_y),(test_X, test_y)],verbose=100,early_stopping_rounds=e_stoping_r)\n",
    "    else:\n",
    "        model.fit(train_X,train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:29: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:32: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:36: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:39: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\n",
    "from sklearn.utils import resample,shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "seed=1024\n",
    "np.random.seed(seed)\n",
    "path = \"../input/\"\n",
    "train = pd.read_csv(path+\"train_porter.csv\")\n",
    "\n",
    "\n",
    "# tfidf\n",
    "train_question1_tfidf = pd.read_pickle(path+'train_question1_tfidf.pkl')[:]\n",
    "#test_question1_tfidf = pd.read_pickle(path+'test_question1_tfidf.pkl')[:]\n",
    "\n",
    "train_question2_tfidf = pd.read_pickle(path+'train_question2_tfidf.pkl')[:]\n",
    "#test_question2_tfidf = pd.read_pickle(path+'test_question2_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_question1_porter_tfidf = pd.read_pickle(path+'train_question1_porter_tfidf.pkl')[:]\n",
    "#test_question1_porter_tfidf = pd.read_pickle(path+'test_question1_porter_tfidf.pkl')[:]\n",
    "\n",
    "train_question2_porter_tfidf = pd.read_pickle(path+'train_question2_porter_tfidf.pkl')[:]\n",
    "#test_question2_porter_tfidf = pd.read_pickle(path+'test_question2_porter_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_interaction = pd.read_pickle(path+'train_interaction.pkl')[:].reshape(-1,1)\n",
    "#test_interaction = pd.read_pickle(path+'test_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_interaction = pd.read_pickle(path+'train_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "#test_porter_interaction = pd.read_pickle(path+'test_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "train_jaccard = pd.read_pickle(path+'train_jaccard.pkl')[:].reshape(-1,1)\n",
    "#test_jaccard = pd.read_pickle(path+'test_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_jaccard = pd.read_pickle(path+'train_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "#test_porter_jaccard = pd.read_pickle(path+'test_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_len = pd.read_pickle(path+\"train_len.pkl\")\n",
    "test_len = pd.read_pickle(path+\"test_len.pkl\")\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.vstack([train_len,test_len]))\n",
    "train_len = scaler.transform(train_len)\n",
    "#test_len =scaler.transform(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#abhitest=pd.read_csv(\"../rader/test_features.csv\")\n",
    "\n",
    "abhitrain=pd.read_csv(\"../rader/train_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abhife=['fuzz_qratio','fuzz_WRatio','fuzz_partial_token_set_ratio','fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd', 'cityblock_distance', 'canberra_distance', 'euclidean_distance', 'minkowski_distance', 'braycurtis_distance', 'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec']\n",
    "#abhitest[abhife].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#abhitest[abhife]=abhitest[abhife].replace(np.inf, np.nan)\n",
    "#abhitest[abhife]=abhitest[abhife].fillna(0)\n",
    "\n",
    "abhitrain[abhife]=abhitrain[abhife].replace(np.inf, np.nan)\n",
    "abhitrain[abhife]=abhitrain[abhife].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_porter_w2vecsim = np.array(pd.read_pickle(path+'train_porter_w2vecsim.pkl'))[:].reshape(-1,1)\n",
    "train_porter_w2vecdest = np.array(pd.read_pickle(path+'train_porter_w2vecdist.pkl'))[:].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_porter_w2vecsim = np.array(pd.read_csv(path+'wors2vecsim_test.csv')['w2v_sim']).reshape(-1,1)\n",
    "#test_porter_w2vecdest = np.array(pd.read_csv(path+'wors2vecsim_test.csv')['w2v_dist']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_cosinesim=pd.read_pickle(path+\"train_porter_cosine_dist.pkl\")[:].reshape(-1,1)\n",
    "#test_consine_sim=pd.read_pickle(path+\"test_porter_cosine_dist.pkl\")[:].reshape(-1,1)\n",
    "y = train['is_duplicate'].values[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"../input/wordvecmodels/w2vecsimvectors/\"\n",
    "glove42B300d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.42B.300d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B100d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.100d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove6B200d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.200d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove840B300d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.840B.300d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B200d_test_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.200d_test_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove6B50d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.50d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B100d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.100d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B200d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.twitter.27B.200d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B200d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.twitter.27B.200d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove42B300d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.42B.300d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B100d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.twitter.27B.100d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B200d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.200d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove6B50d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.50d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove840B300d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.840B.300d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B100d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.twitter.27B.100d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_q1_svd=pd.read_pickle(\"../input/wordvecmodels/svd/train_q1_svd.pkl\")\n",
    "train_q2_svd=pd.read_pickle(\"../input/wordvecmodels/svd/train_q2_svd.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove840B300d_train_porter_w2vecvector_diff=pd.read_pickle(path+\"glove.840B.300d_train_porter_w2vecvector_diff.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ssp.hstack([\n",
    "    glove840B300d_train_porter_w2vecvector_diff,\n",
    "\ttrain_question1_tfidf,\n",
    "    train_question2_tfidf,\n",
    "    train_interaction,\n",
    "    train_porter_interaction,\n",
    "    train_jaccard,\n",
    "    train_porter_jaccard,\n",
    "    train_len,\n",
    "    ]).tocsr()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "preds, model = runlgbm(X_train, y_train,X_test,y_test,num_rounds=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "om scipy.sparse import csr_matrix\n",
    "path=\"/home/udit/ipython/notebook/quora/input/input/FMModel/stacknet/\"\n",
    "fromsparsetofile(path+\"train1.sparse\", X, deli1=\" \", deli2=\":\",ytarget=y)    \n",
    "fromsparsetofile(path+\"test1.sparse\", X_t, deli1=\" \", deli2=\":\",ytarget=None)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.191269277687\n",
      "0.191144081052\n"
     ]
    }
   ],
   "source": [
    "path=\"/home/udit/ipython/notebook/quora/input/input/FMModel/input/\"\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=seed).split(X)\n",
    "for ind_tr, ind_te in skf:\n",
    "    X_train = X[ind_tr]\n",
    "    X_test = X[ind_te]\n",
    "\n",
    "    y_train = y[ind_tr]\n",
    "    y_test = y[ind_te]\n",
    "    break\n",
    "\n",
    "dump_svmlight_file(X,y,path+\"X_tfidf.svm\")\n",
    "#del X\n",
    "dump_svmlight_file(X_t,np.zeros(X_t.shape[0]),path+\"X_t_tfidf.svm\")\n",
    "#del X_t\n",
    "\n",
    "def oversample(X_ot,y,p=0.165):\n",
    "    pos_ot = X_ot[y==1]\n",
    "    neg_ot = X_ot[y==0]\n",
    "    #p = 0.165\n",
    "    scale = ((pos_ot.shape[0]*1.0 / (pos_ot.shape[0] + neg_ot.shape[0])) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_ot = ssp.vstack([neg_ot, neg_ot]).tocsr()\n",
    "        scale -=1\n",
    "    neg_ot = ssp.vstack([neg_ot, neg_ot[:int(scale * neg_ot.shape[0])]]).tocsr()\n",
    "    ot = ssp.vstack([pos_ot, neg_ot]).tocsr()\n",
    "    y=np.zeros(ot.shape[0])\n",
    "    y[:pos_ot.shape[0]]=1.0\n",
    "    print y.mean()\n",
    "    return ot,y\n",
    "\n",
    "X_train,y_train = oversample(X_train.tocsr(),y_train,p=0.165)\n",
    "X_test,y_test = oversample(X_test.tocsr(),y_test,p=0.165)\n",
    "\n",
    "X_train,y_train = shuffle(X_train,y_train,random_state=seed)\n",
    "\n",
    "dump_svmlight_file(X_train,y_train,path+\"X_train_tfidf.svm\")\n",
    "dump_svmlight_file(X_test,y_test,path+\"X_test_tfidf.svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):    \n",
    "    zsparse=csr_matrix(array)\n",
    "    indptr = zsparse.indptr\n",
    "    indices = zsparse.indices\n",
    "    data = zsparse.data\n",
    "    print(\" data lenth %d\" % (len(data)))\n",
    "    print(\" indices lenth %d\" % (len(indices)))    \n",
    "    print(\" indptr lenth %d\" % (len(indptr)))\n",
    "    \n",
    "    f=open(filename,\"w\")\n",
    "    counter_row=0\n",
    "    for b in range(0,len(indptr)-1):\n",
    "        #if there is a target, print it else , print nothing\n",
    "        if ytarget!=None:\n",
    "             f.write(str(ytarget[b]) + deli1)     \n",
    "             \n",
    "        for k in range(indptr[b],indptr[b+1]):\n",
    "            if (k==indptr[b]):\n",
    "                if np.isnan(data[k]):\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))                    \n",
    "            else :\n",
    "                if np.isnan(data[k]):\n",
    "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))  \n",
    "                else :\n",
    "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n",
    "        f.write(\"\\n\")\n",
    "        counter_row+=1\n",
    "        if counter_row%10000==0:    \n",
    "            print(\" row : %d \" % (counter_row))    \n",
    "    f.close()  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
