{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "a = 0.165 / 0.37\n",
    "b = (1 - 0.165) / (1 - 0.37)\n",
    "\n",
    "def kappa(preds, y):\n",
    "    score = []\n",
    "    for pp,yy in zip(preds, y.get_label()):\n",
    "        score.append(a * yy * np.log (pp) + b * (1 - yy) * np.log(1-pp))\n",
    "    score = -np.sum(score) / len(score)\n",
    "\n",
    "    return 'kappa', float(score),False\n",
    "\n",
    "fun_loss  = make_scorer(kappa, greater_is_better=False)\n",
    "\n",
    "def runlgbm(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=500000,e_stoping_r=50): \n",
    "    t4_params = {\n",
    "        'boosting_type': 'gbdt', 'objective': 'binary', 'nthread': -1, 'silent': True,\n",
    "        'num_leaves': 6, 'learning_rate': 0.03, 'max_depth': 6,\n",
    "        'max_bin': 255, 'subsample_for_bin': 50000,\n",
    "        'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.7, 'reg_alpha':1, 'reg_lambda':0,\n",
    "        'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight': 1.36}\n",
    "\n",
    "    # they can be used directly to build a LGBMClassifier (which is wrapped in a sklearn fashion)\n",
    "    model = lgbm.sklearn.LGBMClassifier(n_estimators=num_rounds, seed=0, **t4_params)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        model.fit(train_X,train_y,eval_set=[(train_X,train_y),(test_X, test_y)],verbose=100,early_stopping_rounds=e_stoping_r)\n",
    "    else:\n",
    "        model.fit(train_X,train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    return pred_test_y, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:29: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:30: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:32: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:33: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:36: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:37: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:39: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\n",
    "from sklearn.utils import resample,shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "seed=1024\n",
    "np.random.seed(seed)\n",
    "path = \"../input/\"\n",
    "train = pd.read_csv(path+\"train_porter.csv\")\n",
    "\n",
    "\n",
    "# tfidf\n",
    "train_question1_tfidf = pd.read_pickle(path+'train_question1_tfidf.pkl')[:]\n",
    "test_question1_tfidf = pd.read_pickle(path+'test_question1_tfidf.pkl')[:]\n",
    "\n",
    "train_question2_tfidf = pd.read_pickle(path+'train_question2_tfidf.pkl')[:]\n",
    "test_question2_tfidf = pd.read_pickle(path+'test_question2_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_question1_porter_tfidf = pd.read_pickle(path+'train_question1_porter_tfidf.pkl')[:]\n",
    "test_question1_porter_tfidf = pd.read_pickle(path+'test_question1_porter_tfidf.pkl')[:]\n",
    "\n",
    "train_question2_porter_tfidf = pd.read_pickle(path+'train_question2_porter_tfidf.pkl')[:]\n",
    "test_question2_porter_tfidf = pd.read_pickle(path+'test_question2_porter_tfidf.pkl')[:]\n",
    "\n",
    "\n",
    "train_interaction = pd.read_pickle(path+'train_interaction.pkl')[:].reshape(-1,1)\n",
    "test_interaction = pd.read_pickle(path+'test_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_interaction = pd.read_pickle(path+'train_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "test_porter_interaction = pd.read_pickle(path+'test_porter_interaction.pkl')[:].reshape(-1,1)\n",
    "\n",
    "\n",
    "train_jaccard = pd.read_pickle(path+'train_jaccard.pkl')[:].reshape(-1,1)\n",
    "test_jaccard = pd.read_pickle(path+'test_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_porter_jaccard = pd.read_pickle(path+'train_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "test_porter_jaccard = pd.read_pickle(path+'test_porter_jaccard.pkl')[:].reshape(-1,1)\n",
    "\n",
    "train_len = pd.read_pickle(path+\"train_len.pkl\")\n",
    "test_len = pd.read_pickle(path+\"test_len.pkl\")\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.vstack([train_len,test_len]))\n",
    "train_len = scaler.transform(train_len)\n",
    "test_len =scaler.transform(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abhitest=pd.read_csv(\"../rader/test_features.csv\")\n",
    "\n",
    "abhitrain=pd.read_csv(\"../rader/train_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abhife=abhitrain.columns.tolist()\n",
    "abhife.remove('question1')\n",
    "abhife.remove('question2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abhife=['fuzz_qratio','fuzz_WRatio','fuzz_partial_token_set_ratio','fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd', 'cityblock_distance', 'canberra_distance', 'euclidean_distance', 'minkowski_distance', 'braycurtis_distance', 'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec']\n",
    "#abhitest[abhife].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abhitest[abhife]=abhitest[abhife].replace(np.inf, np.nan)\n",
    "abhitest[abhife]=abhitest[abhife].fillna(0)\n",
    "\n",
    "abhitrain[abhife]=abhitrain[abhife].replace(np.inf, np.nan)\n",
    "abhitrain[abhife]=abhitrain[abhife].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"../input/\"\n",
    "train_porter_w2vecsim =np.array(pd.read_pickle(path+'train_porter_w2vecsim.pkl'))[:].reshape(-1,1)\n",
    "train_porter_w2vecdest = np.array(pd.read_pickle(path+'train_porter_w2vecdist.pkl'))[:].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_porter_w2vecsim = np.array(pd.read_csv(path+'wors2vecsim_test.csv')['w2v_sim']).reshape(-1,1)\n",
    "test_porter_w2vecdest = np.array(pd.read_csv(path+'wors2vecsim_test.csv')['w2v_dist']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "path=\"../input/\"\n",
    "train_cosinesim=pd.read_pickle(path+\"train_porter_cosine_dist.pkl\")[:].reshape(-1,1)\n",
    "test_cosinesim=pd.read_pickle(path+\"test_porter_cosine_dist.pkl\")[:].reshape(-1,1)\n",
    "y = train['is_duplicate'].values[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"../input/wordvecmodels/w2vecsimvectors/\"\n",
    "glove42B300d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.42B.300d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B100d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.100d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove6B200d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.200d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove840B300d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.840B.300d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B50d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.50d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B100d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.100d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B200d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.twitter.27B.200d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B200d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.twitter.27B.200d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove42B300d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.42B.300d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B100d_train_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.twitter.27B.100d_train_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B200d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.200d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove6B50d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.50d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove840B300d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.840B.300d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B100d_train_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.twitter.27B.100d_train_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#glove42B300d_test_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.42B.300d_test_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B100d_test_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.100d_test_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove6B200d_test_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.200d_test_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "#glove840B300d_test_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.840B.300d_test_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B50d_test_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.50d_test_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B100d_test_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.6B.100d_test_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B200d_test_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.twitter.27B.200d_test_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B200d_test_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.twitter.27B.200d_test_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "#glove42B300d_test_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.42B.300d_test_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B100d_test_porter_w2vecsim=np.array(pd.read_pickle(path+\"glove.twitter.27B.100d_test_porter_w2vecsim.pkl\"))[:].reshape(-1,1)\n",
    "glove6B200d_test_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.200d_test_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glove6B50d_test_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.6B.50d_test_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "#glove840B300d_test_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.840B.300d_test_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n",
    "glovetwitter27B100d_test_porter_w2vecdist=np.array(pd.read_pickle(path+\"glove.twitter.27B.100d_test_porter_w2vecdist.pkl\"))[:].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_q1_svd=pd.read_pickle(\"../input/wordvecmodels/svd/train_q1_svd.pkl\")\n",
    "#train_q2_svd=pd.read_pickle(\"../input/wordvecmodels/svd/train_q2_svd.pkl\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_q1_svd=pd.read_pickle(\"../input/train_q1_svd.pkl\")[:]\n",
    "train_q2_svd=pd.read_pickle(\"../input/train_q2_svd.pkl\")[:]\n",
    "train_q1_svd = StandardScaler().fit_transform(train_q1_svd)\n",
    "train_q2_svd = StandardScaler().fit_transform(train_q2_svd)\n",
    "\n",
    "\n",
    "test_q1_svd=pd.read_pickle(\"../input/test_q1_svd.pkl\")[:]\n",
    "test_q2_svd=pd.read_pickle(\"../input/test_q2_svd.pkl\")[:]\n",
    "\n",
    "test_q1_svd = StandardScaler().fit_transform(test_q1_svd)\n",
    "test_q2_svd = StandardScaler().fit_transform(test_q2_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#glove840B300d_train_porter_w2vecvector_diff=pd.read_pickle(path+\"glove.840B.300d_train_porter_w2vecvector_diff.pkl\")\n",
    "glocefeatures_train=np.concatenate([\n",
    "    #glove42B300d_train_porter_w2vecsim,\n",
    "    #glove6B100d_train_porter_w2vecdist,\n",
    "    glove6B200d_train_porter_w2vecsim,\n",
    "    #glove840B300d_train_porter_w2vecsim,\n",
    "    glove6B50d_train_porter_w2vecsim,\n",
    "    glove6B100d_train_porter_w2vecsim,\n",
    "    #glovetwitter27B200d_train_porter_w2vecdist,\n",
    "    glovetwitter27B200d_train_porter_w2vecsim,\n",
    "    #glove42B300d_train_porter_w2vecdist,\n",
    "    glovetwitter27B100d_train_porter_w2vecsim,\n",
    "    #glove6B200d_train_porter_w2vecdist,\n",
    "    #glove6B50d_train_porter_w2vecdist,\n",
    "    #glove840B300d_train_porter_w2vecdist,\n",
    "    glovetwitter27B100d_train_porter_w2vecdist\n",
    "    ],axis=1)\n",
    "\n",
    "glocefeatures_test=np.concatenate([\n",
    "    \n",
    "    #glove42B300d_test_porter_w2vecsim,\n",
    "    #glove6B100d_test_porter_w2vecdist,\n",
    "    glove6B200d_test_porter_w2vecsim,\n",
    "    #glove840B300d_test_porter_w2vecsim,\n",
    "    glove6B50d_test_porter_w2vecsim,\n",
    "    glove6B100d_test_porter_w2vecsim,\n",
    "    #glovetwitter27B200d_test_porter_w2vecdist,\n",
    "    glovetwitter27B200d_test_porter_w2vecsim,\n",
    "    #glove42B300d_test_porter_w2vecdist,\n",
    "    glovetwitter27B100d_test_porter_w2vecsim,\n",
    "    #glove6B200d_test_porter_w2vecdist,\n",
    "    #glove6B50d_test_porter_w2vecdist,\n",
    "    #glove840B300d_test_porter_w2vecdist,\n",
    "    glovetwitter27B100d_test_porter_w2vecdist\n",
    "    ],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train['is_duplicate'].values[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ssp.hstack([\n",
    "    train_porter_w2vecsim,\n",
    "    train_porter_w2vecdest,\n",
    "    glocefeatures_train,\n",
    "    csr_matrix(abhitrain[abhife]),\n",
    "    train_cosinesim,\n",
    "    train_question1_tfidf,\n",
    "    train_question2_tfidf,\n",
    "    train_interaction,\n",
    "    train_porter_interaction,\n",
    "    train_jaccard,\n",
    "    train_porter_jaccard,\n",
    "    train_len,\n",
    "    ]).tocsr()\n",
    "\n",
    "X_t = ssp.hstack([\n",
    "    test_porter_w2vecsim,\n",
    "    test_porter_w2vecdest,\n",
    "    glocefeatures_test,\n",
    "    csr_matrix(abhitest[abhife]),\n",
    "    test_cosinesim,\n",
    "    test_question1_tfidf,\n",
    "    test_question2_tfidf,\n",
    "    test_interaction,\n",
    "    test_porter_interaction,\n",
    "    test_jaccard,\n",
    "    test_porter_jaccard,\n",
    "    test_len,\n",
    "    ]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_lgb_native(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=500000,e_stoping_r=50):\n",
    "   \n",
    "    params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    #'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_freq': 7,\n",
    "    'verbose': 0,\n",
    "    #'regression_l1':107\n",
    "    #'scale_pos_weight':1.36,\n",
    "       # 'is_unbalance':True\n",
    "        }\n",
    "    if test_y is not None:\n",
    "        lgb_train = lgbm.Dataset(train_X, train_y,\n",
    "                        free_raw_data=False)\n",
    "        lgb_eval = lgbm.Dataset(test_X, test_y, reference=lgb_train,\n",
    "                        free_raw_data=False)\n",
    "        model = lgbm.train(params,lgb_train, num_boost_round=num_rounds, feval=kappa,valid_sets=lgb_eval,verbose_eval=10)\n",
    "        #model.fit(train_X,train_y,eval_set=[(train_X,train_y),(test_X, test_y)],verbose=100,early_stopping_rounds=e_stoping_r,eval_metric=fun_loss)\n",
    "    else:\n",
    "        lgb_train = lgbm.Dataset(train_X, train_y,\n",
    "                        free_raw_data=False)\n",
    "        model=lgbm.train(params,lgb_train,num_boost_round=num_rounds, feval=kappa)\n",
    "        \n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_test_y, model\n",
    "\n",
    "def kappa1(preds, y):\n",
    "    score = []\n",
    "    for pp,yy in zip(preds, y.get_label()):\n",
    "        score.append(a * yy * np.log (pp) + b * (1 - yy) * np.log(1-pp))\n",
    "    score = -np.sum(score) / len(score)\n",
    "\n",
    "    return 'kappa', float(score)\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=321, num_rounds=2000):\n",
    "    param = {}\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param['eta'] = 0.02\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['eval_metric'] = \"logloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20,verbose_eval=50,feval=kappa1)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds,feval=kappa1)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "preds, model = run_lgb_native(X_train, y_train,X_test,y_test,num_rounds=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds, model = run_lgb_native(X, y,X_t,num_rounds=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds, model = run_lgb_native(X, y,X_t,num_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "om scipy.sparse import csr_matrix\n",
    "path=\"/home/udit/ipython/notebook/quora/input/input/FMModel/stacknet/\"\n",
    "fromsparsetofile(path+\"train1.sparse\", X, deli1=\" \", deli2=\":\",ytarget=y)    \n",
    "fromsparsetofile(path+\"test1.sparse\", X_t, deli1=\" \", deli2=\":\",ytarget=None)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.191269277687\n",
      "0.191144081052\n"
     ]
    }
   ],
   "source": [
    "path=\"/home/udit/ipython/notebook/quora/input/input/FMModel/input/\"\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=seed).split(X)\n",
    "for ind_tr, ind_te in skf:\n",
    "    X_train = X[ind_tr]\n",
    "    X_test = X[ind_te]\n",
    "\n",
    "    y_train = y[ind_tr]\n",
    "    y_test = y[ind_te]\n",
    "    break\n",
    "\n",
    "dump_svmlight_file(X,y,path+\"X_tfidf.svm\")\n",
    "#del X\n",
    "dump_svmlight_file(X_t,np.zeros(X_t.shape[0]),path+\"X_t_tfidf.svm\")\n",
    "#del X_t\n",
    "\n",
    "def oversample(X_ot,y,p=0.165):\n",
    "    pos_ot = X_ot[y==1]\n",
    "    neg_ot = X_ot[y==0]\n",
    "    #p = 0.165\n",
    "    scale = ((pos_ot.shape[0]*1.0 / (pos_ot.shape[0] + neg_ot.shape[0])) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_ot = ssp.vstack([neg_ot, neg_ot]).tocsr()\n",
    "        scale -=1\n",
    "    neg_ot = ssp.vstack([neg_ot, neg_ot[:int(scale * neg_ot.shape[0])]]).tocsr()\n",
    "    ot = ssp.vstack([pos_ot, neg_ot]).tocsr()\n",
    "    y=np.zeros(ot.shape[0])\n",
    "    y[:pos_ot.shape[0]]=1.0\n",
    "    print y.mean()\n",
    "    return ot,y\n",
    "\n",
    "X_train,y_train = oversample(X_train.tocsr(),y_train,p=0.165)\n",
    "X_test,y_test = oversample(X_test.tocsr(),y_test,p=0.165)\n",
    "\n",
    "X_train,y_train = shuffle(X_train,y_train,random_state=seed)\n",
    "\n",
    "dump_svmlight_file(X_train,y_train,path+\"X_train_tfidf.svm\")\n",
    "dump_svmlight_file(X_test,y_test,path+\"X_test_tfidf.svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):    \n",
    "    zsparse=csr_matrix(array)\n",
    "    indptr = zsparse.indptr\n",
    "    indices = zsparse.indices\n",
    "    data = zsparse.data\n",
    "    print(\" data lenth %d\" % (len(data)))\n",
    "    print(\" indices lenth %d\" % (len(indices)))    \n",
    "    print(\" indptr lenth %d\" % (len(indptr)))\n",
    "    \n",
    "    f=open(filename,\"w\")\n",
    "    counter_row=0\n",
    "    for b in range(0,len(indptr)-1):\n",
    "        #if there is a target, print it else , print nothing\n",
    "        if ytarget!=None:\n",
    "             f.write(str(ytarget[b]) + deli1)     \n",
    "             \n",
    "        for k in range(indptr[b],indptr[b+1]):\n",
    "            if (k==indptr[b]):\n",
    "                if np.isnan(data[k]):\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))                    \n",
    "            else :\n",
    "                if np.isnan(data[k]):\n",
    "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))  \n",
    "                else :\n",
    "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n",
    "        f.write(\"\\n\")\n",
    "        counter_row+=1\n",
    "        if counter_row%10000==0:    \n",
    "            print(\" row : %d \" % (counter_row))    \n",
    "    f.close()  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
